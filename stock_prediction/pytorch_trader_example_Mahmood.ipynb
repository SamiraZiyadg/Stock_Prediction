{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"Python 3.7.6 64-bit ('base': conda)","display_name":"Python 3.7.6 64-bit ('base': conda)","metadata":{"interpreter":{"hash":"b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"}}},"colab":{"name":"pytorch_trader_example_Mahmood.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"HCxegk6vasjU"},"source":["# Pytorch Example\n","\n","This notebook walks through essentially **all** the major steps of the submission process. The only step you have to do is create and zip up the files (main.py, agent.py, and any models or other files you need) and submit them to our website.\n","\n","Specifically, this notebook will...\n","\n","- Load train.csv into a pandas dataframe\n","- Train a RL agent using our custom gym environment* \n","- Save model to disk \n","- Show how to write an agent.py file to use model (see sample agent.py for details)\n","- Show how to write a main.py file to take row as stdin and output (Action, frac) as stdout\n","\n","This notebook does not complete the final steps, namely...\n","\n","- Actually create agent.py \n","- Actually create main.py\n","- Zip agent.py, main.py, and any model files together and submit on \\<insert website url\\>\n","- Note your score and try again!\n","\n","*See util.py or the deep_stock_trader_custom_environment notebook for more details\n","\n","---\n","\n","The model trained here is super basic (and the code has a nasty buy somewhere causing it to only HOLD when evaluated...). Attend Josiah's RL course to learn more. You can also start playing with the network's architecture and the training hyperparameters. Maybe adjust the reward function in the DeepStockTraderEnv. Are there ways to include context/memory? So much to explore!\n","\n","***PLEASE SEE THE END OF THIS NOTEBOOK WHICH OUTLINES HOW TO SUBMIT TO OUR WEBSITE!!***\n","\n","---\n","\n","Good luck, <br>\n","Seth Hamilton | TAMU Datathon R&D"]},{"cell_type":"code","metadata":{"id":"V8gErxhMauHa","executionInfo":{"status":"ok","timestamp":1603022627154,"user_tz":240,"elapsed":2867,"user":{"displayName":"Mahmood Tabesh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7cBxHJG3EhEpMbb6b_pOJeafzbTWJzDaYZt7b=s64","userId":"03924614200557524243"}},"outputId":"2024955e-67b5-41ba-cf01-05ae291fb79c","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["pip install torch"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eieTuVNdasjV","executionInfo":{"status":"ok","timestamp":1603022627564,"user_tz":240,"elapsed":3267,"user":{"displayName":"Mahmood Tabesh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7cBxHJG3EhEpMbb6b_pOJeafzbTWJzDaYZt7b=s64","userId":"03924614200557524243"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import pandas as pd\n","from util import Action, DeepStockTraderEnv\n","from collections import namedtuple  \n","import math\n","import random\n","import matplotlib.pyplot as plt\n","from IPython import display"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u_D4B1WKasjZ"},"source":["## Define the network"]},{"cell_type":"code","metadata":{"id":"9ase5Ix0asjZ","executionInfo":{"status":"ok","timestamp":1603022627569,"user_tz":240,"elapsed":3263,"user":{"displayName":"Mahmood Tabesh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7cBxHJG3EhEpMbb6b_pOJeafzbTWJzDaYZt7b=s64","userId":"03924614200557524243"}}},"source":["class TraderNetwork(nn.Module):\n","    def __init__(self, row_size):\n","        super(TraderNetwork, self).__init__()\n","\n","        self.size = row_size\n","\n","        self.fc1 = nn.Linear(row_size, row_size*2)\n","        self.fc2 = nn.Linear(row_size*2, row_size*4)\n","        self.fc3 = nn.Linear(row_size*4, row_size)\n","        self.fc4 = nn.Linear(row_size, 3)\n","\n"," #       self.fc1 = nn.Linear(row_size, 128)\n"," #       self.fc2 = nn.Linear(128, 64)\n","  #      self.fc3 = nn.Linear(64, 3)\n","\n","    def forward(self, t):\n","        # Have the tensor \"flow\" through the network\n","        t = F.relu(self.fc1(t))\n","        t = F.relu(self.fc2(t))\n","        t = F.relu(self.fc3(t))\n","        t = F.relu(self.fc4(t))\n","\n","        return t"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xbTULbSfasjb"},"source":["## Define helper classes\n","\n","Much of the following code was copied from DeepLizard.com's RL pytorch course. I highly recommend it. \n","\n","Check it out here: [RL pytorch course](https://deeplizard.com/learn/playlist/PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv)"]},{"cell_type":"code","metadata":{"id":"HDVzB3h3asjc","executionInfo":{"status":"ok","timestamp":1603022627570,"user_tz":240,"elapsed":3255,"user":{"displayName":"Mahmood Tabesh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7cBxHJG3EhEpMbb6b_pOJeafzbTWJzDaYZt7b=s64","userId":"03924614200557524243"}}},"source":["def extract_tensors(experiences):\n","    # Convert batch of Experiences to Experience of batches\n","    batch = Experience(*zip(*experiences))\n","\n","    t1 = torch.cat(batch.state).to(device, dtype=torch.double)\n","    t2 = torch.cat(batch.action).to(device, dtype=torch.int64)\n","    t3 = torch.cat(batch.reward).to(device, dtype=torch.double)\n","    t4 = torch.cat(batch.next_state).to(device, dtype=torch.double)\n","\n","    return (t1,t2,t3,t4)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"jLZHAzdhasje","executionInfo":{"status":"ok","timestamp":1603022627570,"user_tz":240,"elapsed":3247,"user":{"displayName":"Mahmood Tabesh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7cBxHJG3EhEpMbb6b_pOJeafzbTWJzDaYZt7b=s64","userId":"03924614200557524243"}}},"source":["class QValues():\n","    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    device = torch.device(\"cuda\")\n","\n","    @staticmethod\n","    def get_current(policy_net, states, actions):\n","        return policy_net(states).gather(dim=1, index=actions)\n","\n","    @staticmethod        \n","    def get_next(target_net, next_states):  \n","        return target_net(next_states).max(dim=1)[0]"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"i2dl_YZeasjh","executionInfo":{"status":"ok","timestamp":1603022627571,"user_tz":240,"elapsed":3240,"user":{"displayName":"Mahmood Tabesh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7cBxHJG3EhEpMbb6b_pOJeafzbTWJzDaYZt7b=s64","userId":"03924614200557524243"}}},"source":["Experience = namedtuple(\n","    'Experience',\n","    ('state', 'action', 'next_state', 'reward')\n",")"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"nriZXOgiasjj","executionInfo":{"status":"ok","timestamp":1603022627571,"user_tz":240,"elapsed":3233,"user":{"displayName":"Mahmood Tabesh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7cBxHJG3EhEpMbb6b_pOJeafzbTWJzDaYZt7b=s64","userId":"03924614200557524243"}}},"source":["class ReplayMemory():\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.push_count = 0\n","\n","    def push(self, experience):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(experience)\n","        else:\n","            self.memory[self.push_count % self.capacity] = experience\n","        self.push_count += 1\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def can_provide_sample(self, batch_size):\n","        return len(self.memory) >= batch_size"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ENr1nYXSasjl","executionInfo":{"status":"ok","timestamp":1603022627572,"user_tz":240,"elapsed":3226,"user":{"displayName":"Mahmood Tabesh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7cBxHJG3EhEpMbb6b_pOJeafzbTWJzDaYZt7b=s64","userId":"03924614200557524243"}}},"source":["class EpsilonGreedyStrategy():\n","    def __init__(self, start, end, decay, device):\n","        self.start = start\n","        self.end = end\n","        self.decay = decay\n","        self.device = device\n","\n","    def get_exploration_rate(self, current_step):\n","        return self.end + (self.start - self.end) * \\\n","            math.exp(-1. * current_step * self.decay)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xdyc1CxOasjn","executionInfo":{"status":"ok","timestamp":1603022627572,"user_tz":240,"elapsed":3218,"user":{"displayName":"Mahmood Tabesh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7cBxHJG3EhEpMbb6b_pOJeafzbTWJzDaYZt7b=s64","userId":"03924614200557524243"}}},"source":["class TrainingAgent:\n","    def __init__(self, strategy, num_actions, device):\n","        self.current_step = 0\n","        self.strategy = strategy\n","        self.num_actions = num_actions\n","        self.device = device\n","        self.rate=0\n","\n","    def select_action(self, state, policy_net):\n","        self.rate = self.strategy.get_exploration_rate(self.current_step)\n","        self.current_step += 1\n","\n","        if self.rate > random.random():\n","            return random.randrange(self.num_actions) # explore      \n","        else:\n","            with torch.no_grad():\n","                return policy_net(state).argmax(dim=0).item() # exploit    \n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LOm3f3M2asjp"},"source":["## Environment/Data setup"]},{"cell_type":"code","metadata":{"id":"3I0XO-FAasjq","executionInfo":{"status":"ok","timestamp":1603022627777,"user_tz":240,"elapsed":3416,"user":{"displayName":"Mahmood Tabesh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7cBxHJG3EhEpMbb6b_pOJeafzbTWJzDaYZt7b=s64","userId":"03924614200557524243"}}},"source":["df = pd.read_csv(\"./mystery_stock_daily_train.csv\")\n","env = DeepStockTraderEnv(df)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R0ZRBsvmasjs"},"source":["## Hyperparameters and Training Loop"]},{"cell_type":"code","metadata":{"id":"nK8k7UYBasjt","executionInfo":{"status":"ok","timestamp":1603022627778,"user_tz":240,"elapsed":3407,"user":{"displayName":"Mahmood Tabesh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7cBxHJG3EhEpMbb6b_pOJeafzbTWJzDaYZt7b=s64","userId":"03924614200557524243"}}},"source":["# Hyperparameters (Feel free to tune)\n","batch_size = 32\n","gamma = 0.99\n","eps_start = 1\n","eps_end = 0.001\n","eps_decay = 0.00001\n","target_update = 32\n","memory_size = 100000\n","lr = 0.001\n","n_episodes = 10000"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"zLedv4H_asjv","executionInfo":{"status":"ok","timestamp":1603022627778,"user_tz":240,"elapsed":3392,"user":{"displayName":"Mahmood Tabesh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7cBxHJG3EhEpMbb6b_pOJeafzbTWJzDaYZt7b=s64","userId":"03924614200557524243"}}},"source":["def get_moving_average(period, values):\n","    values = torch.tensor(values, dtype=torch.float)\n","    if len(values) >= period:\n","        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n","            .mean(dim=1).flatten(start_dim=0)\n","        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n","        return moving_avg.numpy()\n","    else:\n","        moving_avg = torch.zeros(len(values))\n","        return moving_avg.numpy()\n","\n","def plot(values, moving_avg_period):\n","    plt.clf()        \n","    plt.title('Training...')\n","    plt.xlabel('Episode')\n","    plt.ylabel('Total Money USD')\n","    plt.plot(values)\n","    plt.plot(get_moving_average(moving_avg_period, values))\n","    plt.show()"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"0dAZnpU8asjx","outputId":"732dc7d3-172c-49b5-ba02-2765b2c32f44","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Other important setup\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device = torch.device(\"cuda\")\n","\n","strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay, device)\n","t_agent = TrainingAgent(strategy, env.action_space.n, device)\n","memory = ReplayMemory(memory_size)\n","\n","policy_net = TraderNetwork(env.row_size).to(device, dtype=torch.double)\n","target_net = TraderNetwork(env.row_size).to(device, dtype=torch.double)\n","\n","target_net.load_state_dict(policy_net.state_dict())\n","target_net.eval()\n","\n","optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n","\n","episode_values = []\n","max_value = 0\n","\n","# TRAINING LOOP\n","for episode in range(n_episodes):\n","    state = env.reset()\n","    state = torch.tensor(state, dtype=torch.double, device=device)\n","\n","    display.clear_output(wait=True)\n","    plot(episode_values, 100)\n","    \n","    list_state_actions = []\n","    timestep = 0\n","    while True:\n","        action = t_agent.select_action(state, policy_net)\n","        # print(action)\n","\n","        next_state, reward, done, info = env.step(Action(action))\n","        next_state = torch.tensor(next_state, dtype=torch.double, device=device)\n","\n","        action_t = torch.tensor([action]).unsqueeze(0)\n","        reward_t = torch.tensor([reward]).unsqueeze(0)\n","\n","        memory.push(Experience(state.unsqueeze(0), action_t, next_state.unsqueeze(0), reward_t))\n","        state = next_state\n","\n","        if memory.can_provide_sample(batch_size):\n","            experiences = memory.sample(batch_size)\n","            states, actions, rewards, next_states = extract_tensors(experiences)\n","            list_state_actions.append([states, actions, rewards, next_states])\n","\n","\n","\n","            current_q_values = QValues.get_current(policy_net, states, actions)\n","            next_q_values = QValues.get_next(target_net, next_states)\n","            target_q_values = (next_q_values * gamma) + rewards\n","\n","            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        if done:\n","            total = env.total(timestep=-1, open=False)\n","            episode_values.append(total)\n","            if total > max_value:\n","                max_value = total\n","                torch.save(policy_net.state_dict(), \"./example_model_mahmood.pt\")\n","            break\n","        #print(t_agent.rate)\n","        timestep += 1\n","\n","    if episode % target_update == 0:\n","        target_net.load_state_dict(policy_net.state_dict())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Buffered data was truncated after reaching the output size limit."],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xIFabSvOasjz"},"source":["## Sample Submission Construction\n","\n","Remember: You need to submit a zip file containing\n","- main.py\n","- agent.py\n","- example_model.pt (or the name of your model file)"]},{"cell_type":"markdown","metadata":{"id":"LyHjC1o1asj0"},"source":["### Example agent.py"]},{"cell_type":"code","metadata":{"id":"F15PUKFHasj0"},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","import numpy as np\n","from enum import IntEnum\n","\n","class Action(IntEnum):\n","    BUY = 0\n","    SELL = 1\n","    HOLD = 2\n","\n","# You need to include any network definitions\n","class TraderNetwork(nn.Module):\n","    def __init__(self, row_size):\n","        super(TraderNetwork, self).__init__()\n","\n","        self.size = row_size\n","\n","        # self.fc1 = nn.Linear(row_size, row_size*2)\n","        # self.fc2 = nn.Linear(row_size*2, row_size)\n","        # self.fc3 = nn.Linear(row_size, 3)\n","\n","\n","        self.fc1 = nn.Linear(row_size, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 3)\n","\n","    def forward(self, t):\n","        # Have the tensor \"flow\" through the network\n","        # t = F.relu(self.fc1(t))\n","        # t = F.relu(self.fc2(t))\n","        # t = F.relu(self.fc3(t))\n","\n","        t = F.relu(self.fc1(t))\n","        t = F.relu(self.fc2(t))\n","        t = F.relu(self.fc3(t))\n","\n","\n","        return t\n","\n","class Agent:\n","    def __init__(self, row_size):\n","        \"\"\"\n","        Write your custom initialization sequence here.\n","        This can include loading models from file.\n","        \"\"\"\n","        self.tn = TraderNetwork(row_size).double()        \n","        self.tn.load_state_dict(torch.load(\"./example_model_mahmood.pt\"))   \n","             \n","        self.tn.eval()\n","\n","    def step(self, row):\n","        \"\"\"\n","        Make a decision to be executed @ the open of the next timestep. \n","\n","        row is a numpy array with the same format as the training data\n","\n","        Return a tuple (Action, fraction). Fraction means different \n","        things for different actions...\n","        \n","        Action.BUY:  represents fraction of cash to spend on purchase \n","        Action.SELL: represents fraction of owned shares to sell \n","        Action.HOLD: value ignored.\n","\n","        See the code below on how to return\n","        \"\"\"\n","\n","        t = torch.tensor(row)\n","        choice = torch.argmax(self.tn(t).squeeze(0)).item()\n","\n","        # The plan was to never have to use constants...\n","        # Yeah, we're assuming consistency in buy=0, sell=1, and hold=2\n","        if choice == 0:\n","            return (Action.BUY, 1)\n","        elif choice == 1:\n","            return (Action.SELL, 1)\n","\n","        return (Action.HOLD, 0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"krGg9_Emasj2"},"source":["### Example main.py\n","\n","I call this an \"example\", but literally this is what your main.py file should contain. (You might need to slightly change if if you mess with your agent's constructor or step function interface)"]},{"cell_type":"code","metadata":{"id":"f7ySRx9rasj3"},"source":["import sys\n","from agent_mahmood import Agent\n","\n","a = None\n","for line in sys.stdin:\n","    row = line.split(',')\n","    row = np.array([float(x.strip()) for x in row])\n","    if not a:\n","        a = Agent(len(row))\n","\n","    res = a.step(row)\n","    print(f\"{res[0].name} {res[1]}\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ITsFFYzGasj5"},"source":["## Evaluate your model"]},{"cell_type":"code","metadata":{"tags":[],"id":"ARCmi5mPasj5"},"source":["from util import Evaluation\n","\n","agent = Agent(env.row_size)\n","e = Evaluation(df, 1000, agent)\n","\n","print(\"---------------Evaluation Stats---------------\")\n","print(f\"total:    {e.total()}\")\n","print(f\"cash:     {e.cash}\")\n","print(f\"n_shares: {e.n_shares}\")\n","print(f\"n_buys:   {e.n_buys}\")\n","print(f\"n_sells:  {e.n_sells}\")\n","print(f\"n_holds:  {e.n_holds}\")\n","\n","print(agent.tn)\n","plt.plot(e.account_values)\n","plt.title(\"Account Value over Time\")\n","plt.xlabel(\"Days\")\n","plt.ylabel(\"Money (USD)\")"],"execution_count":null,"outputs":[]}]}